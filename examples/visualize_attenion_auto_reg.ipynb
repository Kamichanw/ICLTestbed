{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Attention on Images with Heat Map\n",
    "In this tutorial, we will show step by step how to use `ICLTestbed` to visualize attention of an auto-regressive architecture model.\n",
    "\n",
    "Let's take LLaVa 1.5 as an example. LLaVa is an auto-regressive architecture multimodal model that takes as input arbitrary sequences of texts and images, and generates text responses. The more details about LLaVa can be found in following ways:\n",
    "\n",
    "[paper](https://arxiv.org/abs/2304.08485) [hf model](https://huggingface.co/llava-hf/llava-1.5-7b-hf) [official-code](https://github.com/haotian-liu/LLaVA)\n",
    "\n",
    "## Step 1. Prepare Model and Inputs\n",
    "The model in ICLTestbed can be roughly regarded as a simple combination of a processor and a specific model. You can access underlying processor or model by `model.processor` or `model.model`.\n",
    "\n",
    "The model input should be a [conversation-like object](https://huggingface.co/docs/transformers/main/en/conversations), which is a format that is easy to understand. The corresponding chat is:\n",
    "\n",
    "<ul>\n",
    "  <li>\n",
    "    User: <span style=\"vertical-align: top;\">What is in this image?</span>\n",
    "    <img src=\"./images/idefix.jpg\" alt=\"Idefix image\" style=\"vertical-align: top;\">\n",
    "  </li>\n",
    "  <li>\n",
    "    Assistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.\n",
    "  </li>\n",
    "  <li>\n",
    "    User: \n",
    "    <img src=\"./images/caesar.png\" alt=\"Caesar image\" style=\"vertical-align: top;\">\n",
    "    <span style=\"vertical-align: top;\">And who is that?</span>\n",
    "  </li>\n",
    "  <li>\n",
    "    Assistant:\n",
    "  </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af3a784c2974ee3981f6fbd877c6d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from testbed.models import LLaVa\n",
    "\n",
    "model_path = \"/data/share/model_weight/llava/llava-1.5-7b-hf\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = LLaVa(model_path, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"And who is that?\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"assistant\"},\n",
    "]\n",
    "\n",
    "images = [\n",
    "    PIL.Image.open(os.path.abspath(os.path.join(\"images\", \"idefix.jpg\"))),\n",
    "    PIL.Image.open(os.path.abspath(os.path.join(\"images\", \"caesar.png\"))),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set a Tracker\n",
    "To visualize attention, we need to get the attention scores over all layers.\n",
    "\n",
    "Here, `testbed` offers various trackers to extract intermediate variables during forward. The tracker is an observer that records module inputs and outputs. \n",
    "\n",
    "For example, you can attach a `testbed.utils.tracker.ForwardTracker`\n",
    "to the model to collect attention inputs and outputs during a forward. This is useful when a HF model is not support `output_attentions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That is Idefix, the dog of Obelix in Asterix and Obelix'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testbed.utils.tracker import ForwardTracker\n",
    "import copy\n",
    "\n",
    "with ForwardTracker() as k_proj, ForwardTracker() as q_proj:\n",
    "    model.add_tracker(r\"language_model.model.layers.\\d+.self_attn.k_proj$\", k_proj)\n",
    "    model.add_tracker(r\"language_model.model.layers.\\d+.self_attn.q_proj$\", q_proj)\n",
    "    results = model.generate(\n",
    "        images,\n",
    "        conversation,\n",
    "        max_new_tokens=20,\n",
    "        return_generated_ids=True,\n",
    "        return_inputs=True,\n",
    "    )\n",
    "    inputs, generated_text, generated_ids = (\n",
    "        results[\"inputs\"],\n",
    "        results[\"outputs\"][0],\n",
    "        results[\"generated_ids\"][0],\n",
    "    )\n",
    "\n",
    "    assert len(generated_ids) == len(k_proj.outputs)\n",
    "    key_states, query_states = copy.deepcopy(k_proj.outputs), copy.deepcopy(\n",
    "        q_proj.outputs\n",
    "    )\n",
    "\n",
    "    # manually split heads\n",
    "    num_head = (\n",
    "        model.config.text_config.num_attention_heads\n",
    "    )  # this may various among different models\n",
    "    for token_idx, (token_key, token_query) in enumerate(zip(key_states, query_states)):\n",
    "        for layer_idx, (layer_key, layer_query) in enumerate(zip(token_key, token_query)):\n",
    "            batch_size, kv_len, d_model = layer_key.shape\n",
    "            key_states[token_idx][layer_idx] = layer_key.view(\n",
    "                batch_size, kv_len, num_head, d_model // num_head\n",
    "            ).transpose(1, 2)\n",
    "            query_states[token_idx][layer_idx] = layer_query.view(\n",
    "                batch_size, kv_len, num_head, d_model // num_head\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "# btw, the correct answer is Caesar\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some models, such as Idefics, an extra rotary embedding is added to key and query after `k_proj` and `q_proj`. In this case, `ForwardTracker` will not be able to obtain the final value of key and query. Therefore, `testbed` introduces `testbed.utils.tracker.LocalsTracker` to track the local variables of a module method.\n",
    "\n",
    "If you decide to use `LocalsTracker`, you should check the source code of that module to figure out the names of the variables you want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That is Idefix, the dog of Obelix in Asterix and Obelix'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testbed.utils.tracker import LocalsTracker\n",
    "import copy\n",
    "\n",
    "with LocalsTracker(\"forward\", [\"key_states\", \"query_states\"]) as kq_tracker:\n",
    "    model.add_tracker(r\"language_model.model.layers.\\d+.self_attn$\", kq_tracker)\n",
    "\n",
    "    results = model.generate(\n",
    "        images,\n",
    "        conversation,\n",
    "        max_new_tokens=20,\n",
    "        return_generated_ids=True,\n",
    "        return_inputs=True,\n",
    "    )\n",
    "    inputs, generated_text, generated_ids = (\n",
    "        results[\"inputs\"],\n",
    "        results[\"outputs\"][0],\n",
    "        results[\"generated_ids\"][0],\n",
    "    )\n",
    "    \n",
    "    # in forward, key states are always concatenated with previous cache\n",
    "    # thus, the last step states is states of the whole sequence\n",
    "    key_states, query_states = copy.deepcopy(\n",
    "        kq_tracker.get(\"key_states\")[-1]\n",
    "    ), copy.deepcopy(kq_tracker.get(\"query_states\"))\n",
    "\n",
    "    # make batch of key states \n",
    "    key_states = [key_states]\n",
    "\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 1239, 128]), torch.Size([1, 32, 1220, 128]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states[0][0].shape, query_states[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Calculate Attention and Visualize\n",
    "Now, let's calculate the attention scores, i.e., $\\textrm{softmax}(\\frac{KQ^\\top}{\\sqrt{d_k}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1239]), torch.Size([32, 32, 1239, 1239]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_new_tokens = len(query_states)\n",
    "num_layer = len(query_states[0])  # the 0th forward\n",
    "batch_size, num_head, kv_len, head_dim = query_states[0][\n",
    "    0\n",
    "].shape  # the 0th forward, the 0th layer\n",
    "assert batch_size == 1, \"batch_size should be 1\"\n",
    "\n",
    "\n",
    "def build_qk(outputs):\n",
    "    # returns: [num_layer, num_head, seq_len, head_dim]\n",
    "    return torch.cat(\n",
    "        [torch.cat(token_states) for token_states in outputs],\n",
    "        dim=2,\n",
    "    ).to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "q = build_qk(query_states)\n",
    "k = build_qk(key_states)\n",
    "attn_weights = torch.softmax(\n",
    "    torch.matmul(q, k.transpose(-2, -1)) / head_dim**0.5, dim=-1\n",
    ")\n",
    "# note that the last token of generated_ids is not used in attention calculation\n",
    "seq_ids = torch.cat([inputs.input_ids.squeeze(), generated_ids[:-1]])\n",
    "# we need to do some compute-intensive operations later\n",
    "seq_ids, attn_weights = seq_ids.to(device), attn_weights.to(device)\n",
    "seq_ids.shape, attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac3222befe34687b539c4f3ef575b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AttnVisualizer(children=(DisplayPanel(children=(VBox(children=(HBox(children=(IntRangeSlider(value=(0, 88), co…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from utils import AttnVisualizer\n",
    "\n",
    "display(AttnVisualizer(attn_weights, seq_ids, model.processor.tokenizer, images, \"<image>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
