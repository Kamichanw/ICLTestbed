{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Attention on Images with Heat Map\n",
    "In this tutorial, we will show step by step how to use `ICLTestbed` to visualize cross attention architecture model attention.\n",
    "\n",
    "Let's take Idefics as an example. Idefics is a cross attention architecture multimodal model that takes as input arbitrary sequences of texts and images, and generates text responses. The more details about Idefics can be found in following ways:\n",
    "\n",
    "[paper](https://arxiv.org/abs/2306.16527) [blog](https://huggingface.co/blog/idefics) [official-code](https://github.com/huggingface/transformers/tree/main/src/transformers/models/idefics)\n",
    "\n",
    "## Step 1. Prepare Model and Inputs\n",
    "The model in ICLTestbed can be roughly regarded as a simple combination of a processor and a specific model. You can access underlying processor or model by `model.processor` or `model.model`.\n",
    "\n",
    "The model input should be a [conversation-like object](https://huggingface.co/docs/transformers/main/en/conversations), which is a format that is easy to understand. The corresponding chat is:\n",
    "\n",
    "<ul>\n",
    "  <li>\n",
    "    User: <span style=\"vertical-align: top;\">What is in this image?</span>\n",
    "    <img src=\"./images/idefix.jpg\" alt=\"Idefix image\" style=\"vertical-align: top;\">\n",
    "  </li>\n",
    "  <li>\n",
    "    Assistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.\n",
    "  </li>\n",
    "  <li>\n",
    "    User: \n",
    "    <img src=\"./images/caesar.png\" alt=\"Caesar image\" style=\"vertical-align: top;\">\n",
    "    <span style=\"vertical-align: top;\">And who is that?</span>\n",
    "  </li>\n",
    "  <li>\n",
    "    Assistant:\n",
    "  </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IdeficsForVisionText2Text has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Instantiating IdeficsAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576d21f5f72944c9b0bcf310557984e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from testbed.models import Idefics\n",
    "\n",
    "model_path = \"/data/share/model_weight/idefics/idefics-9b\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = Idefics(model_path, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# from official blog\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"And who is that?\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"assistant\"},\n",
    "]\n",
    "\n",
    "images = [\n",
    "    PIL.Image.open(os.path.abspath(os.path.join(\"images\", \"idefix.jpg\"))),\n",
    "    PIL.Image.open(os.path.abspath(os.path.join(\"images\", \"caesar.png\"))),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set a Tracker\n",
    "To visualize attention, we need to get the attention scores over all layers.\n",
    "\n",
    "Here, `testbed` offers various trackers to extract intermediate variables during forward. The tracker is an observer that records module inputs and outputs. \n",
    "\n",
    "For example, you can attach a `testbed.utils.tracker.ForwardTracker`\n",
    "to the model to collect attention inputs and outputs during a forward. This is useful when a HF model is not support `output_attentions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is Asterix, the main character of the comic book series Asterix.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testbed.utils.tracker import ForwardTracker\n",
    "import copy\n",
    "\n",
    "with ForwardTracker() as k_proj, ForwardTracker() as q_proj:\n",
    "    model.add_tracker(r\"model.layers.\\d+.self_attn.k_proj$\", k_proj)\n",
    "    model.add_tracker(r\"model.layers.\\d+.self_attn.q_proj$\", q_proj)\n",
    "    results = model.generate(\n",
    "        images,\n",
    "        conversation,\n",
    "        max_new_tokens=20,\n",
    "        return_generated_ids=True,\n",
    "        return_inputs=True,\n",
    "    )\n",
    "    inputs, generated_text, generated_ids = (\n",
    "        results[\"inputs\"],\n",
    "        results[\"outputs\"][0],\n",
    "        results[\"generated_ids\"][0],\n",
    "    )\n",
    "\n",
    "    assert len(generated_ids) == len(k_proj.outputs)\n",
    "    # key_states, query_states: max_new_tokens * num_layers * [batch_size, kv_len, d_model]\n",
    "    key_states, query_states = copy.deepcopy(k_proj.outputs), copy.deepcopy(\n",
    "        q_proj.outputs\n",
    "    )\n",
    "\n",
    "    # manually split heads to [batch_size, num_head, kv_len, head_dim]\n",
    "    num_head = (\n",
    "        model.config.num_attention_heads\n",
    "    )  # this may various among different models\n",
    "    for token_idx, (token_key, token_query) in enumerate(zip(key_states, query_states)):\n",
    "        for layer_idx, (layer_key, layer_query) in enumerate(zip(token_key, token_query)):\n",
    "            batch_size, kv_len, d_model = layer_key.shape\n",
    "            key_states[token_idx][layer_idx] = layer_key.view(\n",
    "                batch_size, kv_len, num_head, d_model // num_head\n",
    "            ).transpose(1, 2)\n",
    "            query_states[token_idx][layer_idx] = layer_query.view(\n",
    "                batch_size, kv_len, num_head, d_model // num_head\n",
    "            ).transpose(1, 2)\n",
    "\n",
    "# btw, the correct answer is Caesar\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some models, such as Idefics, an extra rotary embedding is added to key and query after `k_proj` and `q_proj`. In this case, `ForwardTracker` will not be able to obtain the final value of key and query. Therefore, `testbed` introduces `testbed.utils.tracker.LocalsTracker` to track the local variables of a module method.\n",
    "\n",
    "If you decide to use `LocalsTracker`, you should check the source code of that module to figure out the names of the variables you want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is Asterix, the main character of the comic book series Asterix.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testbed.utils.tracker import LocalsTracker\n",
    "import copy\n",
    "\n",
    "with LocalsTracker(\"forward\", [\"key_states\", \"query_states\"]) as kq_tracker:\n",
    "    model.add_tracker(r\"model.layers.\\d+.self_attn$\", kq_tracker)\n",
    "\n",
    "    results = model.generate(\n",
    "        images,\n",
    "        conversation,\n",
    "        max_new_tokens=20,\n",
    "        return_generated_ids=True,\n",
    "        return_inputs=True,\n",
    "    )\n",
    "    inputs, generated_text, generated_ids = (\n",
    "        results[\"inputs\"],\n",
    "        results[\"outputs\"][0],\n",
    "        results[\"generated_ids\"][0],\n",
    "    )\n",
    "\n",
    "    # in forward, key states are always concatenated with previous cache\n",
    "    # thus, the last step states is states of the whole sequence\n",
    "    key_states, query_states = copy.deepcopy(\n",
    "        kq_tracker.get(\"key_states\")[-1]\n",
    "    ), copy.deepcopy(kq_tracker.get(\"query_states\"))\n",
    "\n",
    "    # make batch of key states \n",
    "    key_states = [key_states]\n",
    "\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Calculate Attention and Visualize\n",
    "Now, let's calculate the attention scores, i.e., $\\textrm{softmax}(\\frac{KQ^\\top}{\\sqrt{d_k}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([81]), torch.Size([32, 32, 81, 81]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_new_tokens = len(query_states)\n",
    "num_layer = len(query_states[0])  # the 0th forward\n",
    "batch_size, num_head, kv_len, head_dim = query_states[0][\n",
    "    0\n",
    "].shape  # the 0th forward, the 0th layer\n",
    "assert batch_size == 1, \"batch_size should be 1\"\n",
    "\n",
    "\n",
    "def build_qk(outputs):\n",
    "    # returns: [num_layer, num_head, seq_len, head_dim]\n",
    "    return torch.cat(\n",
    "        [torch.cat(token_states) for token_states in outputs],\n",
    "        dim=2,\n",
    "    ).to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "q = build_qk(query_states)\n",
    "k = build_qk(key_states)\n",
    "attn_weights = torch.softmax(\n",
    "    torch.matmul(q, k.transpose(-2, -1)) / head_dim**0.5, dim=-1\n",
    ")\n",
    "# please note that the last token of generated_ids is not used in attention calculation\n",
    "seq_ids = torch.cat([inputs.input_ids.squeeze(), generated_ids[:-1]]).cpu()\n",
    "seq_ids.shape, attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from utils import AttnVisualizer\n",
    "\n",
    "display(AttnVisualizer(attn_weights, seq_ids, model.processor.tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
